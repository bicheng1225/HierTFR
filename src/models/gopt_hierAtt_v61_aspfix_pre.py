# -*- coding: utf-8 -*-
# @Author  : Bi-Cheng Yan
# @Affiliation  : National Taiwan Normal University
# @Email   : 80847001s@ntnu.edu.tw

# attention and mask prediction are borrowed from espnet (https://github.com/espnet/espnet).

import math
import warnings
import torch
import torch.nn as nn
import numpy as np

import torch.nn.functional as F

from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention
from espnet.nets.pytorch_backend.transformer.label_smoothing_loss import (  # noqa: H301
    LabelSmoothingLoss,
)
from espnet.nets.pytorch_backend.maskctc.add_mask_token import mask_uniform
from espnet.nets.pytorch_backend.nets_utils import (
    th_accuracy,
)

# code from the t2t-vit paper
def get_sinusoid_encoding(n_position, d_hid):
    ''' Sinusoid position encoding table '''

    def get_position_angle_vec(position):
        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]

    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])
    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i
    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1

    return torch.FloatTensor(sinusoid_table).unsqueeze(0)


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = qk_scale or head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        #print(C)
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x):
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

# standard GOPT model proposed in the paper
class GOPT_hierAtt(nn.Module):
    def make_word_pos_mask(self, ys_pad):
        from espnet.nets.pytorch_backend.nets_utils import pad_list

        #0 is mask, 1 value
        B = ys_pad.size()[0]
        L = ys_pad.size()[1]
        ys_mask = torch.zeros(B,L,L)
        for i in range(B):
            for idx, pos in enumerate(ys_pad[i]):
                #-1 padding symbol
                if pos == -1: 
                    break
                ys_mask[i][idx] = (ys_pad[i]==pos).int() 
        return ys_mask

    def __init__(self, embed_dim, num_heads, depth, input_dim=84):
        super().__init__()
        self.input_dim = input_dim
        self.embed_dim = embed_dim
        self.phn_loss = torch.nn.CrossEntropyLoss(ignore_index=0)
        self.word_loss = torch.nn.CrossEntropyLoss(ignore_index=0)
        self.utt_loss = torch.nn.CrossEntropyLoss(ignore_index=0)
        #0:pad, 1:large, 2:equal, 3:small
        self.utt_predi_ln = nn.Linear(embed_dim*2, 4)

        # Transformer encode blocks
        self.phn_blocks = nn.ModuleList([Block(dim=embed_dim, num_heads=1) for i in range(depth)])
        self.word_blocks = nn.ModuleList([Block(dim=embed_dim, num_heads=1) for i in range(depth)])
        #self.utt_blocks = nn.ModuleList([Block(dim=embed_dim, num_heads=1) for i in range(depth)])

        self.word_input_att = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        self.word_input_att1 = MultiHeadedAttention(num_heads, embed_dim, 0.1)

        self.word_input_ln = nn.Linear(embed_dim, embed_dim)
        self.word_input_ln1 = nn.Linear(embed_dim, embed_dim)
        self.word_input_ln2 = nn.Linear(self.input_dim, embed_dim)

        self.utt_input_ln = nn.Linear(embed_dim, embed_dim)
        self.utt_input_ln1 = nn.Linear(embed_dim, embed_dim)
        self.utt_input_ln2 = nn.Linear(embed_dim, embed_dim)
        self.utt_input_ln3 = nn.Linear(self.input_dim, embed_dim)

        # sin pos embedding or learnable pos embedding, 55 = 50 sequence length + 5 utt-level cls tokens
        # self.pos_embed = nn.Parameter(get_sinusoid_encoding(55, self.embed_dim) * 0.1, requires_grad=True)
        self.pos_embed = nn.Parameter(torch.zeros(1, 55, embed_dim))
        trunc_normal_(self.pos_embed, std=.02)

        # sin pos embedding or learnable pos embedding, 55 = 50 sequence length + 5 utt-level cls tokens
        # self.pos_embed = nn.Parameter(get_sinusoid_encoding(55, self.embed_dim) * 0.1, requires_grad=True)
        self.word_cls_pos_embed = nn.Parameter(torch.zeros(1, 5, embed_dim))
        self.word_pos_embed = torch.nn.Embedding(50, embed_dim, padding_idx=0)
        trunc_normal_(self.word_cls_pos_embed, std=.02)

        # for phone classification
        self.in_proj = nn.Linear(self.input_dim, embed_dim)
        self.in_word_proj = nn.Linear(self.input_dim, embed_dim)
        self.in_utt_proj = nn.Linear(self.input_dim, embed_dim)

        self.mlp_head_phn = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))
        
        # for word classification, 1=accuracy, 2=stress, 3=total
        self.mlp_head_word1 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))
        self.mlp_head_word2 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))
        self.mlp_head_word3 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))

        self.aspect_word1 = nn.Linear(embed_dim, embed_dim)
        self.aspect_gate_word1 = nn.Linear(embed_dim*3, embed_dim)
        self.aspect_res_word1 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res2_word1 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res3_word1 = nn.Linear(embed_dim, embed_dim)
        self.aspect_word2 = nn.Linear(embed_dim, embed_dim)
        self.aspect_gate_word2 = nn.Linear(embed_dim*3, embed_dim)
        self.aspect_res_word2 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res2_word2 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res3_word2 = nn.Linear(embed_dim, embed_dim)
        self.aspect_word3 = nn.Linear(embed_dim, embed_dim)
        self.aspect_gate_word3 = nn.Linear(embed_dim*3, embed_dim)
        self.aspect_res_word3 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res2_word3 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res3_word3 = nn.Linear(embed_dim, embed_dim)

        self.asp_att_w1 = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        self.asp_att_w2 = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        self.asp_att_w3 = MultiHeadedAttention(num_heads, embed_dim, 0.1)

        # canonical phone projection, assume there are 40+2 phns, 0 for padding, 41 mask
        self.phn_proj = nn.Linear(42, embed_dim)
        self.phn_predi_ln = nn.Linear(embed_dim, 42)

        # word projection, assume there are 2607 words, 0:2603(2604)+1(pad) +1 (mask)
        self.word_proj = nn.Linear(2607, embed_dim)
        self.word_predi_ln = nn.Linear(embed_dim, 2607)

        # utterance level, 1=accuracy, 2=completeness, 3=fluency, 4=prosodic, 5=total score

        self.gop2utt = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        
        self.utt_input_att = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        self.utt_input_att1 = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        self.utt_input_att2 = MultiHeadedAttention(num_heads, embed_dim, 0.1)

        self.utt2utt = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        
        self.word_fuse_ln = nn.Linear(embed_dim, embed_dim)
        self.word_sem = nn.Linear(embed_dim, 1)
        #self.word_sem2 = nn.Linear(embed_dim, 1)
        #self.word_sem3 = nn.Linear(embed_dim, 1)
        self.cls_token1 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.mlp_head_utt1 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))
        self.aspect_utt1 = nn.Linear(embed_dim, embed_dim)
        self.aspect_gate_utt1 = nn.Linear(embed_dim*5, embed_dim)
        self.aspect_res_utt1 = nn.Linear(embed_dim, embed_dim)
        self.aspect_utt_phn1 = nn.Linear(embed_dim, embed_dim)
        self.aspect_utt_word1 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res2_utt1 = nn.Linear(embed_dim, embed_dim)

        self.cls_token2 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.mlp_head_utt2 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))
        self.aspect_utt2 = nn.Linear(embed_dim, embed_dim)
        self.aspect_gate_utt2 = nn.Linear(embed_dim*5, embed_dim)
        self.aspect_utt_phn2 = nn.Linear(embed_dim, embed_dim)
        self.aspect_utt_word2 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res_utt2 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res2_utt2 = nn.Linear(embed_dim, embed_dim)
        
        self.cls_token3 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.mlp_head_utt3 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))
        self.aspect_utt3 = nn.Linear(embed_dim, embed_dim)
        self.aspect_gate_utt3 = nn.Linear(embed_dim*5, embed_dim)
        self.aspect_utt_phn3 = nn.Linear(embed_dim, embed_dim)
        self.aspect_utt_word3 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res_utt3 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res2_utt3 = nn.Linear(embed_dim, embed_dim)
        
        self.cls_token4 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.mlp_head_utt4 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))
        self.aspect_utt4 = nn.Linear(embed_dim, embed_dim)
        self.aspect_gate_utt4 = nn.Linear(embed_dim*5, embed_dim)
        self.aspect_utt_phn4 = nn.Linear(embed_dim, embed_dim)
        self.aspect_utt_word4 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res_utt4 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res2_utt4 = nn.Linear(embed_dim, embed_dim)
        
        self.cls_token5 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.mlp_head_utt5 = nn.Sequential(nn.LayerNorm(embed_dim), nn.Linear(embed_dim, 1))
        self.aspect_utt5 = nn.Linear(embed_dim, embed_dim)
        self.aspect_gate_utt5 = nn.Linear(embed_dim*5, embed_dim)
        self.aspect_utt_phn5 = nn.Linear(embed_dim, embed_dim)
        self.aspect_utt_word5 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res_utt5 = nn.Linear(embed_dim, embed_dim)
        self.aspect_res2_utt5 = nn.Linear(embed_dim, embed_dim)

        self.asp_att_u1 = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        self.asp_att_u2 = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        self.asp_att_u3 = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        self.asp_att_u4 = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        self.asp_att_u5 = MultiHeadedAttention(num_heads, embed_dim, 0.1)
        
        self.word_cls_token1 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.word_cls_token2 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.word_cls_token3 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.word_cls_token4 = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.word_cls_token5 = nn.Parameter(torch.zeros(1, 1, embed_dim))


        self.u1_gp = nn.Linear(embed_dim*4, 1)
        self.u1_gw = nn.Linear(embed_dim*4, 1)
        self.u1_gu = nn.Linear(embed_dim*4, 1)

        self.u2_gp = nn.Linear(embed_dim*4, 1)
        self.u2_gw = nn.Linear(embed_dim*4, 1)
        self.u2_gu = nn.Linear(embed_dim*4, 1)

        self.u3_gp = nn.Linear(embed_dim*4, 1)
        self.u3_gw = nn.Linear(embed_dim*4, 1)
        self.u3_gu = nn.Linear(embed_dim*4, 1)

        self.u4_gp = nn.Linear(embed_dim*4, 1)
        self.u4_gw = nn.Linear(embed_dim*4, 1)
        self.u4_gu = nn.Linear(embed_dim*4, 1)

        self.u5_gp = nn.Linear(embed_dim*4, 1)
        self.u5_gw = nn.Linear(embed_dim*4, 1)
        self.u5_gu = nn.Linear(embed_dim*4, 1)
        
        # initialize the cls tokens
        trunc_normal_(self.cls_token1, std=.02)
        trunc_normal_(self.cls_token2, std=.02)
        trunc_normal_(self.cls_token3, std=.02)
        trunc_normal_(self.cls_token4, std=.02)
        trunc_normal_(self.cls_token5, std=.02)

        # initialize the cls tokens
        trunc_normal_(self.word_cls_token1, std=.02)
        trunc_normal_(self.word_cls_token2, std=.02)
        trunc_normal_(self.word_cls_token3, std=.02)
        trunc_normal_(self.word_cls_token4, std=.02)
        trunc_normal_(self.word_cls_token5, std=.02)

    # x shape in [batch_size, sequence_len, feat_dim]
    # phn in [batch_size, seq_len]
    def forward(self, x, phn, word, word_pos, utt_acc):

        # batch size
        B = x.shape[0]

        # 1.1 prepare for phone-level input and mask predition
        #ys_pad, self.mask_token, self.eos, self.ignore_id
        ys_in_pad, ys_out_pad = mask_uniform(
            phn.long(), 40, -1, -1
        )
        #41:mask, 0:pad/ignore_id
        ys_in_pad = ys_in_pad +1
        ys_out_pad = ys_out_pad +1
        # get batch length
        L = ys_in_pad.shape[1]


        # phn_one_hot in shape [batch_size, seq_len, feat_dim]
        phn_one_hot = torch.nn.functional.one_hot(ys_in_pad.long(), num_classes=42).float()
        # phn_embed in shape [batch_size, seq_len, embed_dim]
        phn_embed = self.phn_proj(phn_one_hot)

        # phone-level input feats prepare
        # if the input dimension is different from the Transformer embedding dimension, project the input to same dim
        if self.embed_dim != self.input_dim:
            gop_feats = x[:,:L]
            x = self.in_proj(x)[:,:L]
            word_gop = self.in_word_proj(gop_feats)[:,:L]
            utt_gop = self.in_utt_proj(gop_feats)[:,:L]
        x = x + phn_embed

        cls_token1 = self.cls_token1.expand(B, -1, -1)
        cls_token2 = self.cls_token2.expand(B, -1, -1)
        cls_token3 = self.cls_token3.expand(B, -1, -1)
        cls_token4 = self.cls_token4.expand(B, -1, -1)
        cls_token5 = self.cls_token5.expand(B, -1, -1)


        x = torch.cat((cls_token1, cls_token2, cls_token3, cls_token4, cls_token5, x), dim=1)

        x = x + self.pos_embed[:,:L+5,:]

        # phn-level feats extraction: forward to the Transformer encoder
        for blk in self.phn_blocks:
            x = blk(x)

        phn_u1 = x[:, 0]
        phn_u2 = x[:, 1]
        phn_u3 = x[:, 2]
        phn_u4 = x[:, 3]
        phn_u5 = x[:, 4]

        phn_pred_pad = self.phn_predi_ln(x)
        #ignore first 5 token
        phn_pred_pad = phn_pred_pad[:, 5:]

        # 6th-end tokens are phone score tokens
        p = self.mlp_head_phn(x[:, 5:])

        # 1.2 compute phone-level mask predition
        phn_loss_att = self.phn_loss(phn_pred_pad.reshape(-1, 42), ys_out_pad.view(-1))
        
        phn_acc = th_accuracy(
            phn_pred_pad.reshape(-1, 42), ys_out_pad, ignore_label=0
        )

        # 2.1 word-level input feats prepare and prepare mask prediction
        word_ys_in_pad, word_ys_out_pad = mask_uniform(
            word.long(), 2605, -1, -1
        )
        #2606:mask, 0:pad/ignore_id
        word_ys_in_pad = word_ys_in_pad +1
        word_ys_out_pad = word_ys_out_pad +1

        # word-level input feats prepare
        # word_one_hot in shape [batch_size, seq_len, feat_dim]
        word_one_hot = torch.nn.functional.one_hot(word_ys_in_pad.long(), num_classes=2607).float()
        # word_embed in shape [batch_size, seq_len, embed_dim]
        word_embed = self.word_proj(word_one_hot)

        phn2word_msk = self.make_word_pos_mask(word_pos[:, :L])
        phn2word_msk = phn2word_msk.to(word_gop.device)
        
        word_gop = self.word_input_att(word_gop, word_gop, word_gop, phn2word_msk)
        word_feats_phn = self.word_input_att1(x[:, 5:], x[:, 5:], x[:, 5:], phn2word_msk)[:, :L]

        # word-level input feats: phn_gop_feats + phn_aspsect_feats + word_id_embeds
        word_feats = self.word_input_ln(word_gop) + self.word_input_ln1(word_feats_phn) + self.word_input_ln2(gop_feats) + word_embed
        #word_feats = word_feats_phn + word_embed
        word_cls_token1 = self.word_cls_token1.expand(B, -1, -1)
        word_cls_token2 = self.word_cls_token2.expand(B, -1, -1)
        word_cls_token3 = self.word_cls_token3.expand(B, -1, -1)
        word_cls_token4 = self.word_cls_token4.expand(B, -1, -1)
        word_cls_token5 = self.word_cls_token4.expand(B, -1, -1)

        word_feats = torch.cat((word_cls_token1, word_cls_token2, word_cls_token3,
         word_cls_token4, word_cls_token5, word_feats), dim=1)

        # add word level pos_embeds
        word_feats[:, 5:] = word_feats[:, 5:] + self.word_pos_embed(word_pos.int()+1)[:, :L]
        word_feats[:, :5] = word_feats[:, :5] + self.word_cls_pos_embed

        # word-level feats extraction: forward to the Transformer encoder
        for blk in self.word_blocks:
            word_feats = blk(word_feats)

        # 2.2 compute word-level mask predition
        word_pred_pad = self.word_predi_ln(word_feats)[:, 5:]
        word_loss_att = self.word_loss(word_pred_pad.reshape(-1, 2607), word_ys_out_pad.view(-1))
        
        word_acc = th_accuracy(
            word_pred_pad.reshape(-1, 2607), word_ys_out_pad, ignore_label=0
        )

        # keep the model architecture for APA model training 
        # the first 5 tokens are utterance-level cls tokens, i.e., accuracy, completeness, fluency, prosodic, total scores
        w_u1 = word_feats[:, 0]
        w_u2 = word_feats[:, 1]
        w_u3 = word_feats[:, 2]
        w_u4 = word_feats[:, 3]
        w_u5 = word_feats[:, 4]

        # word score is propagated to phone-level, so word output is also at phone-level.
        # but different mlp heads are used, 1 = accuracy, 2 = stress, 3 = total
        word_feats_aspect1 = self.aspect_word1(word_feats[:, 5:])
        word_feats_aspect2 = self.aspect_word2(word_feats[:, 5:])
        word_feats_aspect3 = self.aspect_word3(word_feats[:, 5:])

        word_cat_feat = torch.cat([word_feats_aspect1, word_feats_aspect2, word_feats_aspect3], dim=-1)
        word_att_feats_aspect1 = word_feats_aspect1 * torch.sigmoid(self.aspect_gate_word1(word_cat_feat))
        word_att_feats_aspect2 = word_feats_aspect2 * torch.sigmoid(self.aspect_gate_word2(word_cat_feat))
        word_att_feats_aspect3 = word_feats_aspect3 * torch.sigmoid(self.aspect_gate_word3(word_cat_feat))
        
        #aspect cross att
        # padding elements are Ture
        tgt_msk = (phn.long()==-1)[:, :L]
        # padding elements are False, [25, 50]
        tgt_msk = (~tgt_msk).int()
        ## creat aspect atten mask
        word_asp_mask = torch.zeros(B, tgt_msk.size(1), tgt_msk.size(1)*3).to(phn.device)
        #[w_asp1, w_asp2, w_asp3]
        for l in range(tgt_msk.size(1)):
            word_asp_mask[:,l,0+l], word_asp_mask[:,l, tgt_msk.size(1)+l], word_asp_mask[:,l,tgt_msk.size(1)*2+l]= tgt_msk[:, l], tgt_msk[:, l], tgt_msk[:, l]
        asp_collect = torch.cat([word_att_feats_aspect1, word_att_feats_aspect2, word_att_feats_aspect3], dim=1)

        word_att_feats_aspect1 = self.asp_att_w1(word_att_feats_aspect1, asp_collect, 
            asp_collect, word_asp_mask)
        word_att_feats_aspect2 = self.asp_att_w2(word_att_feats_aspect2, asp_collect, 
            asp_collect, word_asp_mask)
        word_att_feats_aspect3 = self.asp_att_w3(word_att_feats_aspect3, asp_collect, 
            asp_collect, word_asp_mask)
        
        word_feats_aspect1 = self.aspect_res_word1(word_feats_aspect1) + self.aspect_res3_word1(word_att_feats_aspect1) + self.aspect_res2_word1(x[:, 5:])
        word_feats_aspect2 = self.aspect_res_word2(word_feats_aspect2) + self.aspect_res3_word2(word_att_feats_aspect2) + self.aspect_res2_word2(x[:, 5:])
        word_feats_aspect3 = self.aspect_res_word3(word_feats_aspect3) + self.aspect_res3_word3(word_att_feats_aspect3) + self.aspect_res2_word3(x[:, 5:])
        
        w1 = self.mlp_head_word1(word_feats_aspect1)
        w2 = self.mlp_head_word2(word_feats_aspect2)
        w3 = self.mlp_head_word3(word_feats_aspect3)

        # 3.1 utterance-level input feats prepare and pretraining task 
        #utt_msk = (~(phn==-1)).int()
        
        ## creat aspect atten mask
        utt_msk = torch.zeros(B, L, L).to(phn.device)
        for idx, value in enumerate(phn[:, :L]):
            utt_msk[idx] = (~(value==-1)).int()

        pooling_msk = (phn != -1).unsqueeze(-1)[:, :L]
        utt_gop = self.utt_input_att(utt_gop, utt_gop, utt_gop, utt_msk)
        utt_wordFeat = self.utt_input_att1(word_feats[:, 5:], word_feats[:, 5:], word_feats[:, 5:], utt_msk)
        utt_phnAsp = self.utt_input_att2(x[:, 5:], x[:, 5:], x[:, 5:], utt_msk)

        utt_gop = utt_gop * pooling_msk
        utt_gop = torch.sum(utt_gop, dim=1)/pooling_msk.sum(dim=1)

        utt_wordFeat = utt_wordFeat * pooling_msk
        utt_wordFeat = torch.sum(utt_wordFeat, dim=1)/pooling_msk.sum(dim=1)

        utt_phnAsp = utt_phnAsp * pooling_msk
        utt_phnAsp = torch.sum(utt_phnAsp, dim=1)/pooling_msk.sum(dim=1)

        utt_feats = self.utt_input_ln(utt_gop) + self.utt_input_ln1(utt_wordFeat) + self.utt_input_ln2(utt_phnAsp)

        u1_asp = self.aspect_utt1(utt_feats) 
        u2_asp = self.aspect_utt2(utt_feats)
        u3_asp = self.aspect_utt3(utt_feats)
        u4_asp = self.aspect_utt4(utt_feats)
        u5_asp = self.aspect_utt5(utt_feats)

        utt_cat_feat = torch.cat([u1_asp,u2_asp,u3_asp,u4_asp,u5_asp], dim=-1)
        u1_asp_att = u1_asp * torch.sigmoid(self.aspect_gate_utt1(utt_cat_feat))
        u2_asp_att = u2_asp * torch.sigmoid(self.aspect_gate_utt2(utt_cat_feat))
        u3_asp_att = u3_asp * torch.sigmoid(self.aspect_gate_utt3(utt_cat_feat))
        u4_asp_att = u4_asp * torch.sigmoid(self.aspect_gate_utt4(utt_cat_feat))
        u5_asp_att = u5_asp * torch.sigmoid(self.aspect_gate_utt5(utt_cat_feat))

        #perform cross trait attention
        asp_collect = torch.stack([u1_asp_att, u2_asp_att, u3_asp_att, u4_asp_att, u5_asp_att], dim=1)
        u1_feats_aspect_att = self.asp_att_u1(u1_asp_att, asp_collect, asp_collect, None).squeeze(1)
        u2_feats_aspect_att = self.asp_att_u2(u2_asp_att, asp_collect, asp_collect, None).squeeze(1)
        u3_feats_aspect_att = self.asp_att_u3(u3_asp_att, asp_collect, asp_collect, None).squeeze(1)
        u4_feats_aspect_att = self.asp_att_u4(u4_asp_att, asp_collect, asp_collect, None).squeeze(1)
        u5_feats_aspect_att = self.asp_att_u5(u5_asp_att, asp_collect, asp_collect, None).squeeze(1)

        u1_asp = self.aspect_res_utt1(u1_asp.squeeze()) + u1_feats_aspect_att
        u2_asp = self.aspect_res_utt2(u2_asp.squeeze()) + u2_feats_aspect_att
        u3_asp = self.aspect_res_utt3(u3_asp.squeeze()) + u3_feats_aspect_att
        u4_asp = self.aspect_res_utt4(u4_asp.squeeze()) + u4_feats_aspect_att
        u5_asp = self.aspect_res_utt5(u5_asp.squeeze()) + u5_feats_aspect_att

        # fusion mechanism
        u1_gp = torch.sigmoid(self.u1_gp(torch.cat([u1_asp, w_u1, phn_u1, utt_gop], dim=-1)))
        u1_gw = torch.sigmoid(self.u1_gw(torch.cat([u1_asp, w_u1, phn_u1, utt_gop], dim=-1)))
        u1_gu = torch.sigmoid(self.u1_gu(torch.cat([u1_asp, w_u1, phn_u1, utt_gop], dim=-1)))
        u1_feat = u1_gp * phn_u1 + u1_gw * w_u1 + u1_gu * u1_asp


        u2_gp = torch.sigmoid(self.u2_gp(torch.cat([u2_asp, w_u2, phn_u2, utt_gop], dim=-1)))
        u2_gw = torch.sigmoid(self.u2_gw(torch.cat([u2_asp, w_u2, phn_u2, utt_gop], dim=-1)))
        u2_gu = torch.sigmoid(self.u2_gu(torch.cat([u2_asp, w_u2, phn_u2, utt_gop], dim=-1)))
        u2_feat = u2_gp * phn_u2 + u2_gw * w_u2 + u2_gu * u2_asp

        u3_gp = torch.sigmoid(self.u3_gp(torch.cat([u3_asp, w_u3, phn_u3, utt_gop], dim=-1)))
        u3_gw = torch.sigmoid(self.u3_gw(torch.cat([u3_asp, w_u3, phn_u3, utt_gop], dim=-1)))
        u3_gu = torch.sigmoid(self.u3_gu(torch.cat([u3_asp, w_u3, phn_u3, utt_gop], dim=-1)))
        u3_feat = u3_gp * phn_u3 + u3_gw * w_u3 + u3_gu * u3_asp

        u4_gp = torch.sigmoid(self.u4_gp(torch.cat([u4_asp, w_u4, phn_u4, utt_gop], dim=-1)))
        u4_gw = torch.sigmoid(self.u4_gw(torch.cat([u4_asp, w_u4, phn_u4, utt_gop], dim=-1)))
        u4_gu = torch.sigmoid(self.u4_gu(torch.cat([u4_asp, w_u4, phn_u4, utt_gop], dim=-1)))
        u4_feat = u4_gp * phn_u4 + u4_gw * w_u4 + u4_gu * u4_asp

        u5_gp = torch.sigmoid(self.u5_gp(torch.cat([u5_asp, w_u5, phn_u5, utt_gop], dim=-1)))
        u5_gw = torch.sigmoid(self.u5_gw(torch.cat([u5_asp, w_u5, phn_u5, utt_gop], dim=-1)))
        u5_gu = torch.sigmoid(self.u5_gu(torch.cat([u5_asp, w_u5, phn_u5, utt_gop], dim=-1)))
        u5_feat = u5_gp * phn_u5 + u5_gw * w_u5 + u5_gu * u5_asp

        u1_feat = u1_feat + self.aspect_res2_utt1(utt_gop) 
        u2_feat = u2_feat + self.aspect_res2_utt2(utt_gop)
        u3_feat = u3_feat + self.aspect_res2_utt3(utt_gop)
        u4_feat = u4_feat + self.aspect_res2_utt4(utt_gop)
        u5_feat = u5_feat + self.aspect_res2_utt5(utt_gop)
        
        u1 = self.mlp_head_utt1(u1_feat)
        u2 = self.mlp_head_utt2(u2_feat)
        u3 = self.mlp_head_utt3(u3_feat)
        u4 = self.mlp_head_utt4(u4_feat)
        u5 = self.mlp_head_utt5(u5_feat)

        # creat competive pairs for pretraining 
        cp_label = []
        cp_utt_index = np.random.choice(len(utt_acc), len(utt_acc))
        cp_acc = utt_acc[cp_utt_index]
        for idx in range(len(cp_utt_index)):
            if cp_acc[idx] > utt_acc[idx]:
               cp_label.append(0)
            if cp_acc[idx] == utt_acc[idx]:
               cp_label.append(1)
            if cp_acc[idx] < utt_acc[idx]:
               cp_label.append(2)
        cp_label = torch.tensor(cp_label).to(utt_acc.device)
        utt_feats = u1_feat+u2_feat+u3_feat+u4_feat+u5_feat
        cp_feats_1, cp_feats_2 = utt_feats[cp_utt_index], utt_feats
        utt_predi_pad = self.utt_predi_ln(torch.cat([cp_feats_1, cp_feats_2], dim=-1))

        cp_label = cp_label.unsqueeze(-1)

        utt_loss = self.utt_loss(utt_predi_pad.reshape(-1, 4), cp_label.view(-1))
        
        utt_acc = th_accuracy(
            utt_predi_pad.reshape(-1, 4), cp_label, ignore_label=0
        )
        
        return phn_loss_att, phn_acc, word_loss_att, word_acc, utt_loss, utt_acc